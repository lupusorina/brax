{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "# Training in Brax\n",
        "\n",
        "Once an environment is created in brax, we can quickly train it using brax's built-in training algorithms. Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 22308,
          "status": "ok",
          "timestamp": 1679686324614,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "kUrAlZTod7t_"
      },
      "outputs": [],
      "source": [
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a GPU runtime.  From the Colab menu, choose Runtime > Change Runtime Type, then select **'GPU'** in the dropdown.\n",
        "\n",
        "import functools\n",
        "import jax\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from jax import numpy as jp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import HTML, clear_output\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "import flax\n",
        "from brax import envs\n",
        "from brax.io import model\n",
        "from brax.io import json\n",
        "from brax.io import html\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.sac import train as sac\n",
        "from brax.training.agents.ddpg import train as ddpg\n",
        "\n",
        "from brax.io import image\n",
        "from PIL import Image\n",
        "import io, base64\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoyyw6pQ3xVJ"
      },
      "source": [
        "First let's pick an environment and a backend to train an agent in. \n",
        "\n",
        "Recall from the [Brax Basics](https://github.com/google/brax/blob/main/notebooks/basics.ipynb) colab, that the backend specifies which physics engine to use, each with different trade-offs between physical realism and training throughput/speed. The engines generally decrease in physical realism but increase in speed in the following order: `generalized`,  `positional`, then `spring`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 480
        },
        "executionInfo": {
          "elapsed": 6143,
          "status": "ok",
          "timestamp": 1679686333304,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "4hHuDp53e4VJ",
        "outputId": "90d4c41b-9a25-4ca4-c6be-93172ea9ef57"
      },
      "outputs": [],
      "source": [
        "#@title Load Env { run: \"auto\" }\n",
        "\n",
        "env_name = 'ant'  # @param ['ant', 'halfcheetah', 'hopper', 'humanoid', 'humanoidstandup', 'inverted_pendulum', 'inverted_double_pendulum', 'pusher', 'reacher', 'walker2d']\n",
        "backend = 'positional'  # @param ['generalized', 'positional', 'spring']\n",
        "\n",
        "env = envs.get_environment(env_name=env_name,\n",
        "                           backend=backend)\n",
        "state = jax.jit(env.reset)(rng=jax.random.PRNGKey(seed=0))\n",
        "\n",
        "HTML(html.render(env.sys, [state.pipeline_state]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMailSDb30t-"
      },
      "source": [
        "# Training\n",
        "\n",
        "Brax provides out of the box the following training algorithms:\n",
        "\n",
        "* [Proximal policy optimization](https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py)\n",
        "* [Soft actor-critic](https://github.com/google/brax/blob/main/brax/training/agents/sac/train.py)\n",
        "* [Evolutionary strategy](https://github.com/google/brax/blob/main/brax/training/agents/es/train.py)\n",
        "* [Analytic policy gradients](https://github.com/google/brax/blob/main/brax/training/agents/apg/train.py)\n",
        "* [Augmented random search](https://github.com/google/brax/blob/main/brax/training/agents/ars/train.py)\n",
        "\n",
        "Trainers take as input an environment function and some hyperparameters, and return an inference function to operate the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3MA1UYlftuq"
      },
      "source": [
        "# Training\n",
        "\n",
        "Let's train the Ant policy using the `generalized` backend with PPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 321
        },
        "executionInfo": {
          "elapsed": 190263,
          "status": "ok",
          "timestamp": 1671658344336,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "FB6G2_Yt4A2m",
        "outputId": "402a0a43-3525-4eca-a425-ffcc71e8db0f"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "\n",
        "# We determined some reasonable hyperparameters offline and share them here.\n",
        "train_fn = {\n",
        "  'inverted_pendulum': functools.partial(ppo.train, num_timesteps=2_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'inverted_double_pendulum': functools.partial(ppo.train, num_timesteps=20_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  #'ant': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=4096, batch_size=2048, seed=1),\n",
        "  'ant': functools.partial(ddpg.train,  num_timesteps=10_000_000, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=5, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=512, seed=1),\n",
        "  'humanoid': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=10, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=1024, seed=1, checkpoint_logdir='/tmp/ant_checkpoints'),\n",
        "  'reacher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=4, unroll_length=50, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=256, max_devices_per_host=8, seed=1),\n",
        "  'humanoidstandup': functools.partial(ppo.train, num_timesteps=100_000_000, num_evals=20, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=15, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=6e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'hopper': functools.partial(sac.train, num_timesteps=6_553_600, num_evals=20, reward_scaling=30, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=512, grad_updates_per_step=64, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
        "  'walker2d': functools.partial(sac.train, num_timesteps=7_864_320, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=128, grad_updates_per_step=32, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
        "  'halfcheetah': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=512, seed=3),\n",
        "  'pusher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=30, num_minibatches=16, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4,entropy_cost=1e-2, num_envs=2048, batch_size=512, seed=3),\n",
        "}[env_name]\n",
        "\n",
        "\n",
        "max_y = {'ant': 8000, 'halfcheetah': 8000, 'hopper': 2500, 'humanoid': 13000, 'humanoidstandup': 75_000, 'reacher': 5, 'walker2d': 5000, 'pusher': 0}[env_name]\n",
        "min_y = {'reacher': -100, 'pusher': -150}.get(env_name, 0)\n",
        "\n",
        "xdata, ydata = [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  xdata.append(num_steps)\n",
        "  ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(xdata, ydata)\n",
        "  plt.show()\n",
        "\n",
        "make_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlh7puy2ZM1"
      },
      "source": [
        "The trainers return an inference function, parameters, and the final set of metrics gathered during evaluation.\n",
        "\n",
        "# Saving and Loading Policies\n",
        "\n",
        "Brax can save and load trained policies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gOWeDqlP35sI"
      },
      "outputs": [],
      "source": [
        "model.save_params('/tmp/params', params)\n",
        "params = model.load_params('/tmp/params')\n",
        "inference_fn = make_inference_fn(params)\n",
        "\n",
        "# half_params = model.load_params('/tmp/ant_checkpoints_ddpg_halfway.pkl')\n",
        "# inference_fn_half = make_inference_fn(half_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YlZvIG320sK"
      },
      "source": [
        "The trainers return an inference function, parameters, and the final set of metrics gathered during evaluation.\n",
        "\n",
        "# Saving and Loading Policies\n",
        "\n",
        "Brax can save and load trained policies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "height": 480
        },
        "executionInfo": {
          "elapsed": 33520,
          "status": "ok",
          "timestamp": 1679346718844,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "kF5fS-yo35sI",
        "outputId": "94d1a7d5-8d6e-456c-8cfd-94a689b8808f"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing a trajectory of the learned inference function\n",
        "\n",
        "# create an env with auto-reset\n",
        "env = envs.create(env_name=env_name, backend=backend)\n",
        "\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "rollout = []\n",
        "rng = jax.random.PRNGKey(seed=42)\n",
        "state = jit_env_reset(rng=rng)\n",
        "for i in range(1000):\n",
        "  rollout.append(state.pipeline_state)\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  act, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_env_step(state, act)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# render each pipeline state to an RGB array\n",
        "frames = image.render_array(env.sys, rollout, height=320, width=320, camera='track')\n",
        "\n",
        "# convert all frames to PIL images\n",
        "frames_pil = [Image.fromarray(f) for f in frames]\n",
        "\n",
        "frames_pil[0].save(\n",
        "    'ant_ddpg.gif',\n",
        "    format='GIF',\n",
        "    append_images=frames_pil[1:],   # the rest of the frames\n",
        "    save_all=True,\n",
        "    duration=1000/30,  # ~30 fps\n",
        "    loop=0,\n",
        ")\n",
        "# write a GIF in-memory\n",
        "buf = io.BytesIO()\n",
        "frames_pil[0].save(\n",
        "    buf, \n",
        "    format='GIF',\n",
        "    append_images=frames_pil[1:], \n",
        "    save_all=True, \n",
        "    duration=1000/30,   # ~30 fps\n",
        "    loop=0\n",
        ")\n",
        "\n",
        "# embed the GIF in the notebook\n",
        "gif_data = buf.getvalue()\n",
        "HTML(f'<img src=\"data:image/gif;base64,{base64.b64encode(gif_data).decode(\"utf-8\")}\"/>')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBtrAqns35sI"
      },
      "source": [
        "🙌 See you soon!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "biped_mathieu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
